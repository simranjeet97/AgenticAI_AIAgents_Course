{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# To Build UI\n",
    "import streamlit as st\n",
    "\n",
    "# For Emebdding Model\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Agno Agentic AI Library to Build AI Agents\n",
    "from agno.agent import Agent\n",
    "from agno.models.ollama import Ollama # Reasoning - Gemma3\n",
    "from agno.models.google import Gemini # Web Search Agent\n",
    "from agno.tools.duckduckgo import DuckDuckGoTools\n",
    "from agno.vectordb.chroma import ChromaDb # RAG\n",
    "\n",
    "# Langchain for Document Parsing and RAG DB Buildng\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# --- Set Google API Key ---\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCr35hxFrpVsbNWgqOwU6PwmkpwLmO2dJA\"\n",
    "\n",
    "# --- Constants ---\n",
    "COLLECTION_NAME = \"Gemma3_rag\"\n",
    "EMBEDDING_MODEL = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") # ✅ Use Google AI Embeddings\n",
    "\n",
    "chroma = ChromaDb(\n",
    "collection=COLLECTION_NAME,\n",
    "path='./chroma_db',\n",
    "embedder=EMBEDDING_MODEL,\n",
    "persistent_client=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    chroma.client.get_collection(name=COLLECTION_NAME)\n",
    "except Exception:\n",
    "\n",
    "    chroma.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insisde\n",
      "Okay, this is a good summary of the document. Here's a breakdown of the key takeaways and a slightly reorganized version focusing on the core information:\n",
      "\n",
      "**Core Summary:**\n",
      "\n",
      "This document details a research paper exploring different Retrieval-Augmented Generation (RAG) approaches for Question Answering (QA) tasks, particularly within the context of a CS dataset. It focuses on evaluating the performance of three RAG models: PathRAG, LightRAG, and GraphRAG. The goal is to determine which approach provides the best results based on comprehension, diversity, logicality, relevance, and coherence.\n",
      "\n",
      "**Key Points & Analysis:**\n",
      "\n",
      "* **RAG Approach:** The paper investigates different RAG techniques – PathRAG, LightRAG, and GraphRAG – each employing a unique strategy for retrieving relevant information.\n",
      "* **Evaluation Dimensions:** The evaluation is based on five key dimensions:\n",
      "    * **Comprehensiveness:** How much detail is provided.\n",
      "    * **Diversity:** How varied the answers are.\n",
      "    * **Logicality:** How well the answers connect.\n",
      "    * **Relevance:** How well the answers address the question.\n",
      "    * **Coherence:** How well the answers maintain logical connections.\n",
      "* **Model Comparison:** The paper compares the performance of PathRAG-lt and LightRAG on a CS dataset, with PathRAG-lt achieving a 50.69% win rate.\n",
      "* **Case Study:** A case study is presented comparing the answers generated by PathRAG and LightRAG on the CS dataset, highlighting the strengths of PathRAG.\n",
      "* **Key Findings:** PathRAG demonstrates clear advantages across all five dimensions, suggesting it's a more robust approach.\n",
      "\n",
      "**Reorganized & Focused Information:**\n",
      "\n",
      "**1. Research Goal:** To evaluate different RAG techniques (PathRAG, LightRAG, GraphRAG) for Question Answering, specifically within the CS dataset.\n",
      "\n",
      "**2. Evaluation Dimensions:** The paper focuses on Comprehensiveness, Diversity, Logicality, Relevance, and Coherence.\n",
      "\n",
      "**3. Model Comparison:**\n",
      "    * **PathRAG-lt:**  Achieved a 50.69% win rate on the CS dataset.\n",
      "    * **LightRAG:** Achieved a 50.69% win rate on the CS dataset.\n",
      "\n",
      "**4. Case Study:** A comparison of the answers generated by PathRAG and LightRAG on the CS dataset, highlighting PathRAG's strengths.\n",
      "\n",
      "**5. Key Takeaway:** PathRAG is a more effective RAG approach, demonstrating superior performance across multiple dimensions.\n",
      "\n",
      "---\n",
      "\n",
      "**Do you want me to elaborate on any of these points, or perhaps provide more detail on a specific aspect of the research? For example, would you like me to:**\n",
      "\n",
      "*   Expand on the specific techniques used in each RAG model?\n",
      "*   Discuss the implications of the win rate for the research?\n",
      "*   Explain the CS dataset in more detail?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def split_texts(documents):\n",
    "  \"\"\"Splits documents into manageable text chunks.\"\"\"\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "  split_docs = text_splitter.split_documents(documents)\n",
    "  return [Document(page_content=chunk.page_content, metadata=chunk.metadata) for chunk in split_docs if chunk.page_content.strip()]\n",
    "\n",
    "def process_pdf(uploaded_file):\n",
    "  \"\"\"Extracts and splits text from an uploaded PDF file and generates embeddings.\"\"\"\n",
    "  print(\"Insisde\")\n",
    "  try:\n",
    "    loader = PyPDFLoader(uploaded_file)\n",
    "    documents = loader.load()\n",
    "\n",
    "    for doc in documents:\n",
    "      doc.metadata.update({\n",
    "        \"source_type\": \"pdf\",\n",
    "        \"file_name\": uploaded_file,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "      })\n",
    "    return split_texts(documents)\n",
    "  except Exception as e:\n",
    "    return []\n",
    "\n",
    "from agno.agent import Agent\n",
    "from agno.models.ollama import Ollama # Reasoning - Gemma3\n",
    "\n",
    "# Update the file path here\n",
    "uploaded_file_path = \"pathrag.pdf\"\n",
    "data = process_pdf(uploaded_file_path)\n",
    "\n",
    "def generate_followup_questions(text):\n",
    "    followup_prompt = f\"\"\"\n",
    "You are an AI assistant.\n",
    "\n",
    "TASK:\n",
    "- Read the following text carefully, Don't go for summarization or overview analysis, just Generate Questions.\n",
    "- Generate exactly **5 questions** that test understanding of the **key points**.\n",
    "- **DO NOT** summarize, explain, or give an overview.\n",
    "- **ONLY** output the 5 questions in a numbered list format.\n",
    "\n",
    "Example format:\n",
    "1. [Question 1]\n",
    "2. [Question 2]\n",
    "3. [Question 3]\n",
    "4. [Question 4]\n",
    "5. [Question 5]\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "    followup_agent = Agent(\n",
    "        name=\"Follow-up Question Agent\",\n",
    "        model=Ollama(\"gemma3:1b\"),\n",
    "        instructions=[\n",
    "            \"Only output 5 numbered questions.\"\n",
    "        ],\n",
    "        markdown=True,\n",
    "    )\n",
    "    \n",
    "    questions = followup_agent.run(followup_prompt).content\n",
    "    return questions\n",
    "\n",
    "# Combine the text from all documents\n",
    "document_text = \" \".join([doc.page_content for doc in data])  # Combine the text\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCr35hxFrpVsbNWgqOwU6PwmkpwLmO2dJA\"\n",
    "print(generate_followup_questions(document_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.16 ('genai_data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9be726cd0f17263164cef21b801912bc8b8bf4ec041a01143391b609d1a24647"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
